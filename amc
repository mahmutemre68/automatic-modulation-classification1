"""
Automatic Modulation Classification (AMC) using Machine Learning
Author: Mahmut Emre Karaman
Description:
    This script generates synthetic wireless signals (BPSK, QPSK, 8PSK)
    under AWGN channel conditions, trains a Random Forest classifier,
    and evaluates performance across different SNR levels.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
import itertools

#Parameters and Settings
NUM_SAMPLES = 1000
SEQ_LEN = 128  # time steps
SNR_RANGE = range(-5, 20, 5)  # -5 dB to 15 dB
RANDOM_SEED = 42


# signal generation functions
def generate_bpsk(n, length):
    # Binary Phase Shift Keying: 2 Symbols (0, 180 degrees)
    bits = np.random.randint(0, 2, (n, length))
    symbols = 2 * bits - 1
    return symbols + 1j * 0


def generate_qpsk(n, length):
    # Quadrature Phase Shift Keying: 4 Symbols
    bits = np.random.randint(0, 4, (n, length))
    phase = (np.pi / 4) + (bits * np.pi / 2)
    symbols = np.exp(1j * phase)
    return symbols


def generate_8psk(n, length):
    # 8-Phase Shift Keying: 8 Symbols
    bits = np.random.randint(0, 8, (n, length))
    phase = bits * (np.pi / 4)
    symbols = np.exp(1j * phase)
    return symbols


def add_awgn(signal, snr_db):
    # Additive White Gaussian Noise (AWGN) simulation
    sig_power = np.mean(np.abs(signal) ** 2)
    snr_linear = 10 ** (snr_db / 10)
    noise_power = sig_power / snr_linear

    # splitting power to I and Q channels
    noise = (np.sqrt(noise_power / 2) * (np.random.randn(*signal.shape) + 1j * np.random.randn(*signal.shape)))
    return signal + noise


# Creating the dataset
print("Simulation starts: Processing data...")
dataset = []
labels = []

for snr in SNR_RANGE:
    # BPSK
    dataset.append(add_awgn(generate_bpsk(NUM_SAMPLES, SEQ_LEN), snr))
    labels.append(np.zeros(NUM_SAMPLES))
    # QPSK
    dataset.append(add_awgn(generate_qpsk(NUM_SAMPLES, SEQ_LEN), snr))
    labels.append(np.ones(NUM_SAMPLES))
    # 8PSK
    dataset.append(add_awgn(generate_8psk(NUM_SAMPLES, SEQ_LEN), snr))
    labels.append(np.full(NUM_SAMPLES, 2))

X_complex = np.vstack(dataset)
y = np.hstack(labels)

# Preprocessing: Complex (I+jQ) -> Real Vector [I, Q]
# ML modeli requires real numbers. I and Q parts getting set side by side.
X_flat = np.concatenate((X_complex.real, X_complex.imag), axis=1)

# splitting train and test sets (%80 train, %20 test)
X_train, X_test, y_train, y_test = train_test_split(X_flat, y, test_size=0.2, random_state=RANDOM_SEED)

print(f"Data ready. Training set: {X_train.shape}")

# Model training (RANDOM FOREST)
print("Model getting trained (Random Forest)...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)
rf_model.fit(X_train, y_train)

# general success rate
y_pred = rf_model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"General Test Success Rate: %{acc * 100:.2f}")

# Visualizing our results
plt.figure(figsize=(12, 5))

# A) Confusion Matrix
plt.subplot(1, 2, 1)
cm = confusion_matrix(y_test, y_pred)
classes = ['BPSK', 'QPSK', '8PSK']
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes);
plt.yticks(tick_marks, classes)

thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment="center",
             color="white" if cm[i, j] > thresh else "black")
plt.ylabel('Real');
plt.xlabel('Prediction')

# B) SNR Performance Analysis (Waterfall Curve)
print("\nSNR Analyzing...")
snr_accuracies = []
sorted_snrs = sorted(list(SNR_RANGE))

for snr in sorted_snrs:
    # new test data for every snr
    test_signals = []
    test_lbls = []

    test_signals.append(add_awgn(generate_bpsk(200, SEQ_LEN), snr));
    test_lbls.append(np.zeros(200))
    test_signals.append(add_awgn(generate_qpsk(200, SEQ_LEN), snr));
    test_lbls.append(np.ones(200))
    test_signals.append(add_awgn(generate_8psk(200, SEQ_LEN), snr));
    test_lbls.append(np.full(200, 2))

    # Preprocessing
    X_loc = np.vstack(test_signals)
    X_loc_flat = np.concatenate((X_loc.real, X_loc.imag), axis=1)
    y_loc = np.hstack(test_lbls)

    # Prediction
    acc_loc = accuracy_score(y_loc, rf_model.predict(X_loc_flat))
    snr_accuracies.append(acc_loc)

plt.subplot(1, 2, 2)
plt.plot(sorted_snrs, snr_accuracies, marker='o', linewidth=2, color='darkorange')
plt.title('Accuracy vs SNR')
plt.xlabel('SNR (dB)');
plt.ylabel('Accuracy')
plt.grid(True)
plt.axhline(y=0.9, color='r', linestyle='--', label='%90 Threshold')
plt.legend()

plt.tight_layout()
plt.show()
